{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.environ.get('LANGCHAIN_API_KEY')\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=os.environ.get('LANGSMITH_ENDPOINT')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=os.environ.get('LANGCHAIN_TRACING_V2')\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.environ.get('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "In a conversational RAG application, queries issued to the retriever should be informed by the context of the conversation. LangChain provides a **create_history_aware_retriever** constructor to simplify this. It constructs a chain that accepts keys input and **chat_history** as input, and has the same output schema as a retriever. **create_history_aware_retriever** requires as inputs:\n",
    "\n",
    "1. LLM\n",
    "2. Retriever\n",
    "3. Prompt\n",
    "   \n",
    "First we obtain these objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Retriever\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = LlamaCloudIndex(\n",
    "  name=\"Schneider_test\", \n",
    "  project_name=\"Default\",\n",
    "  organization_id=\"05d118aa-cdd9-4de1-8cae-58356a536f4c\",\n",
    "  api_key=\"llx-wRERJV3azrzTFlqCjp2KkSRCm3C2VOrHjNWI30sggOTCqiDC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers.llama_index import LlamaIndexRetriever\n",
    "retriever = LlamaIndexRetriever(index=index, query_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(query_kwargs={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x20af6387ef0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What do Discharge lamps depend on?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discharge lamps depend on the luminous electrical discharge through a gas or vapour of a metallic compound, which is contained in a hermetically-sealed transparent envelope at a pre-determined pressure.\n"
     ]
    }
   ],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt\n",
    "\n",
    "### First create a contextualized system prompt\n",
    "\n",
    "We'll use a prompt that includes a MessagesPlaceholder variable under the name \"chat_history\". \n",
    "\n",
    "This allows us to pass in a list of Messages to the prompt using the \"chat_history\" input key, \n",
    "\n",
    "and these messages will be inserted after the system message and before the human message containing the latest question.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "The purpose of this prerequisite chain is to reformulate the question if required in the context of the chat history.\n",
    "\n",
    "If it's not needed. The question is returned as it is.<span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes in 3 things:\n",
    "1. The system Prompt for question Reformulation\n",
    "2. Chat History\n",
    "3. Original Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create History Aware Retriever Chain\n",
    "\n",
    "We can then instantiate the history-aware retriever:\n",
    "\n",
    "This chain prepends a rephrasing of the input query to our retriever, so that the retrieval incorporates the context of the conversation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from pydantic import Field\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "class LlamaQueryEngineRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    A LangChain retriever that wraps a LlamaIndex query engine.\n",
    "    \"\"\"\n",
    "\n",
    "    query_engine: Any = None  # The LlamaIndex query engine\n",
    "\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:\n",
    "        # Run the query through the LlamaIndex query engine\n",
    "        response = self.query_engine.query(query)\n",
    "\n",
    "        # Convert the LlamaIndex response into LangChain Documents\n",
    "        docs = []\n",
    "        for source_node in response.source_nodes:\n",
    "            metadata = source_node.metadata or {}\n",
    "            docs.append(Document(page_content=source_node.get_content(), metadata=metadata))\n",
    "\n",
    "        return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = LlamaQueryEngineRetriever(query_engine=query_engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaQueryEngineRetriever(query_engine=<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x0000020AF6387EF0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever_chain = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build QA Chain\n",
    "\n",
    "As in the RAG tutorial, we will use **create_stuff_documents_chain** to generate a **question_answer_chain**, with input keys \n",
    "\n",
    "1. **context**\n",
    "2. **chat_history**\n",
    "3. **input**\n",
    "\n",
    "It accepts the retrieved context alongside the conversation history and query to generate an answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build RAG Chain\n",
    "\n",
    "\n",
    "We build our final **rag_chain** with **create_retrieval_chain**. \n",
    "\n",
    "This chain applies the \n",
    "**history_aware_retriever_chain** and **question_answer_chain** (created above)\n",
    "\n",
    "in sequence, retaining intermediate outputs such as the retrieved context for convenience. It has input keys input and chat_history, and includes input, chat_history, context, and answer in its output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(history_aware_retriever_chain, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding chat history\n",
    "To manage the chat history, we will need:\n",
    "\n",
    "An object for storing the chat history;\n",
    "An object that wraps our chain and manages updates to the chat history.\n",
    "For these we will use **BaseChatMessageHistory** and **RunnableWithMessageHistory**. The latter is a wrapper for an LCEL chain and a BaseChatMessageHistory that handles injecting chat history into inputs and updating it after each invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neural_ninja\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\schneider-chatbot-uVkHECrX-py3.12\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:775: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `IndexSchema` to V2.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "from langchain_redis import RedisChatMessageHistory\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "#     return RedisChatMessageHistory(\n",
    "#         session_id=session_id,\n",
    "#         redis_url=REDIS_URL\n",
    "#     )\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    return RedisChatMessageHistory(session_id, redis_url=REDIS_URL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a unique session ID\n",
    "import uuid\n",
    "\n",
    "def generate_session_id() -> str:\n",
    "    return uuid.uuid4().hex  # Removes dashes, making it Redis-safe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Conversational RAG Chain\n",
    "\n",
    "\n",
    "Finally we build our final **conversational_rag_chain** with \n",
    "1. rag_chain \n",
    "2. get_session_history\n",
    "3. input_messages_key=\"input\"\n",
    "4. history_messages_key=\"chat_history\"\n",
    "5. output_messages_key=\"answer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a dynamic session ID\n",
    "session_id_1 = generate_session_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id_1=\"9b3556e4ee8547228ea56ae2c5d62225\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"what do Discharge lamps depend on?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id_1}},\n",
    ")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"what is Internal MV distribution?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id_1}},\n",
    ")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your last two questions were: \"Why it depends?\" and \"what are my last 2 questions?\"\n"
     ]
    }
   ],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"what are my last 2 questions?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id_1}},\n",
    ")\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Why it depends?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id_1}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id_1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming final outputs\n",
    "\n",
    "\n",
    "The .stream method will by default stream each key in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream= conversational_rag_chain.stream(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id_1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in stream:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steaming only Answers\n",
    "\n",
    "We are free to process chunks as they are streamed out. If we just want to stream the answer tokens, for example, we can select chunks with the corresponding key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream= conversational_rag_chain.stream(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id_1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream with a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in stream:\n",
    "    if answer_chunk := chunk.get(\"answer\"):\n",
    "        print(f\"{answer_chunk}|\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream without a character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream= conversational_rag_chain.stream(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id_1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in stream:\n",
    "    if answer_chunk := chunk.get(\"answer\"):\n",
    "        print(f\"{answer_chunk}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream the answer Using a .pick method\n",
    " https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.pick\n",
    "More simply, we can use the .pick method to select only the desired key:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream= conversational_rag_chain.stream(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id_1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_answer_chain = conversational_rag_chain.pick(\"answer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream= pick_answer_chain.stream(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id_1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in stream:\n",
    "    print(f\"{chunk}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schneider-chatbot-uVkHECrX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
